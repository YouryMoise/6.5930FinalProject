{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fbc13b-fa37-40a1-9202-eb228c61cc3f",
   "metadata": {},
   "source": [
    "# LoopTree Tutorial\n",
    "\n",
    "LoopTree is a model to evaluate the latency and energy of a fused-layer dataflow accelerator.\n",
    "\n",
    "To model energy and latency, a workload, architecture, and mapping have to be specified. First, we discuss how these are specified. Then, we show how to run the LoopTree model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de3ab36-56ff-471e-8c8a-9a70ac85ee97",
   "metadata": {},
   "source": [
    "## Specifying Architecture, Workload, and Mapping\n",
    "\n",
    "For the LoopTree model to estimate energy and latency, the user must specify an architecture, workload, and mapping. Below, we discuss how to specify each of these inputs.\n",
    "\n",
    "### Architecture\n",
    "In LoopTree, the architecture of an accelerator is abstracted as buffers that use the Buffet semantics and computation units, following the [Timeloop v4 specification](https://timeloop.csail.mit.edu/v4).\n",
    "\n",
    "An example architecture that we will use here is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc6e2cf7-29eb-443d-885a-8c0262ca5edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\n",
      "  version: 0.4\n",
      "  nodes:\n",
      "\n",
      "  - !Component\n",
      "    name: MainMemory\n",
      "    class: DRAM\n",
      "    attributes: {width: 256, block_size: 32, word_bits: 8, datawidth: 8}\n",
      "    required_actions: ['read', 'write']\n",
      "\n",
      "\n",
      "\n",
      "  - !Component\n",
      "    name: IntermediateBuffer\n",
      "    class: SRAM\n",
      "    attributes:\n",
      "      depth: 8192\n",
      "      width: 64\n",
      "      block_size: 32\n",
      "      word_bits: 8\n",
      "      datawidth: 8\n",
      "  \n",
      "  # - !Container\n",
      "  #   name: system_arch\n",
      "  #   attributes:\n",
      "  #     # Top-level attributes inherited by all components unless overridden\n",
      "  #     technology: \"45nm\"\n",
      "  #     global_cycle_seconds: 1e-9\n",
      "  #     datawidth: 16\n",
      "    \n",
      "\n",
      "  - !Component\n",
      "    name: DRAM # offchip DRAM is the source of all datatypes\n",
      "    class: DRAM # assume DRAM is large enough to store all the data, so no depth specification needed\n",
      "    attributes:\n",
      "      width: 64 # width in bits\n",
      "      datawidth: 16\n",
      "    required_actions: ['read', 'write']\n",
      "\n",
      "  # - !Container\n",
      "  #   name: chip\n",
      "\n",
      "  # - !Container\n",
      "  #   name: NNengine\n",
      "  #   spatial: { meshX: 16, meshY: 16 }\n",
      "\n",
      "  - !Component\n",
      "    name: global_buffer\n",
      "    class: SRAM\n",
      "    attributes:\n",
      "      width: 128\n",
      "      depth: 2048\n",
      "      datawidth: 16\n",
      "      n_banks: 1\n",
      "      n_rdwr_ports: 2\n",
      "      n_rd_ports: 0\n",
      "      n_wr_ports: 0\n",
      "    required_actions: ['read', 'write']\n",
      "\n",
      "  - !Container\n",
      "    name: nnEngine\n",
      "    spatial: {meshX: 16, meshY: 16}\n",
      "    attributes:\n",
      "      depth: 16\n",
      "      width: 16\n",
      "      datawidth: 8\n",
      "\n",
      "  # - !Component\n",
      "  #   name: LocalBuffer\n",
      "  #   class: SRAM\n",
      "  #   attributes:\n",
      "  #     depth: 8192\n",
      "  #     width: 256\n",
      "  #     block_size: 32\n",
      "  #     word_bits: 8\n",
      "  #     datawidth: 8\n",
      "  #   required_actions: ['read', 'write']\n",
      "\n",
      "\n",
      "  - !Container\n",
      "    name: PE\n",
      "    spatial: {meshX: 8, meshY: 8}\n",
      "    attributes:\n",
      "      depth: 8\n",
      "      width: 8\n",
      "      datawidth: 8\n",
      "    \n",
      "    \n",
      "  \n",
      "  - !Component \n",
      "    name: input_reg\n",
      "    class: regfile\n",
      "    attributes:\n",
      "      depth: 16\n",
      "      width: 16\n",
      "      datawidth: 8\n",
      "    constraints:\n",
      "      dataspace: {\n",
      "        keep: [Input, OutputVector1, OutputVector2, OutputVector3],\n",
      "        bypass: [Matrix_1, Matrix_2, Matrix_3, Matrix_4, OutputVector4]\n",
      "        }\n",
      "\n",
      "  - !Component \n",
      "    name: weights_reg\n",
      "    class: regfile\n",
      "    attributes:\n",
      "      depth: 16\n",
      "      width: 16\n",
      "      datawidth: 8\n",
      "    constraints:\n",
      "      dataspace: {\n",
      "        keep: [Matrix_1, Matrix_2, Matrix_3, Matrix_4],\n",
      "        bypass: [Input, OutputVector1, OutputVector2, OutputVector3, OutputVector4]\n",
      "        }\n",
      "\n",
      "  - !Component # Output scratchpad\n",
      "    name: output_reg\n",
      "    class: regfile\n",
      "    attributes:\n",
      "      depth: 16\n",
      "      width: 16\n",
      "      datawidth: 8\n",
      "    constraints:\n",
      "      dataspace: {\n",
      "        keep: [OutputVector1, OutputVector2, OutputVector3, OutputVector4],\n",
      "        bypass: [Matrix_1, Matrix_2, Matrix_3, Matrix_4, Input]\n",
      "        }\n",
      "\n",
      "\n",
      "\n",
      "  - !Component\n",
      "    name: MACC\n",
      "    class: intmac\n",
      "    attributes:\n",
      "      datawidth: 8\n",
      "      width: 8\n",
      "      cycle_time: 1e-9\n",
      "    required_actions: ['compute']\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "from pprint import pp\n",
    "show_config('architecture.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9392be-34e6-4d8e-9d96-9e5b3c0bc0c2",
   "metadata": {},
   "source": [
    "### Workloads\n",
    "\n",
    "DNN workloads in LoopTree are abstracted as a set of Einsums, each representing a layer.\n",
    "\n",
    "Each Einsum is specified as a dictionary. Each Einsum reads/writes to a number of tensors (referred to as `data_spaces` in the specification), and each tensor is either an input to the operation or an output (indicated by a key \"read_write\" with value `True`). Moreover, a \"projection\" from the Einsum to each tensor, which describes the index of the element in the data that an operation in the Einsum accesses, must be specified. Finally, we specify must specify the *shape* of the Einsum, which is the bounds of its dimensions (also referred to as \"ranks\").\n",
    "\n",
    "An example comprising two fully-connected layers are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab4d488-7b82-4a9d-8f15-1d3debe11aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_config('two_fc.workload.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45ea13d-6158-44a4-867c-530c52a1cd6a",
   "metadata": {},
   "source": [
    "In this workload specification, we specify two Einsums Fc1 and Fc2. Each Einsum has three dimensions. Fc1 has dimensions P1, M1, and C1; Fc2 has dimensions P2, M2, and C2. Finally, we specify the shape of the Einsums as bounds for each Einsum dimension (rank)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb0c8b9-59e2-4bb4-a6af-83f617b43c69",
   "metadata": {},
   "source": [
    "### Mapping\n",
    "\n",
    "The LoopTree mapping is a tree-structure that contains nodes of the types described below.\n",
    "- **Loops**: a loop node specifies a rank in the Einsum that is partitioned and the shape of the tiles that results from the partitioning. If the loop is a \"temporal\" loop, then the tiles are scheduled to be processed one at a time. If the loop is a \"spatial\" loop, then the tiles are scheduled to parallel hardware units.\n",
    "- **Branching point**: nodes above a branching point (*i.e.*, ancestor nodes) describe inter-Einsum mapping, which is applied to all Einsums under that branching point. Nodes underneath the branching point (*i.e.*, within each branch) pertain only to the Einsum of that branch.\n",
    "- **Storage**: a storage node specifies the tiles of tensors to retain and which hardware level retains the tiles.\n",
    "- **Compute**: a compute node specifies the hardware level used to compute operations of an Einsum. This node has to be a leaf node, and it also denotes the Einsum that a particular branch pertains to.\n",
    "\n",
    "First, we discuss a layer-by-layer mapping example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd28a74-5330-4624-a414-e5ca545984ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_config('layer-by-layer.mapping.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2146686f-b807-491b-af7c-82bfe83bcf53",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- (1.a) The storage node specifies that the tensors specified by the `dspace` key will be retained in target 0, which we will bind to MainMemory.\n",
    "        Note that MainMemory retains Fmap2.\n",
    "- (1.b) The sequential node specifies that the following branches (one for Fc1 and one for Fc2, as we will see shortly) are processed sequentially.\n",
    "- (1.c) Filter1 is retained in GlobalBuffer\n",
    "- (1.d) The rank P1 (which is a rank of the Fc1 Einsum) is partitioned into tiles with shape 1 and we will iterate over the tiles one at a time (temporal iteration).\n",
    "- (1.e) Tiles of Fmap1 and Fmap2 are retained in GlobalBuffer. We retain tiles becuase the P1 rank has been partitioned.\n",
    "- (1.f) Operations of Fc1 are processed in the MACC unit. Moreover, this node specifies that this branch is relevant to Fc2.\n",
    "- (1.g) This and the following nodes specify how Fc2 is mapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3eaa59e-8b83-49ef-8f36-755135d6d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_config('fused.mapping.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f213bff1-447f-4a80-922c-cef4c638d2eb",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- (2.a) Similar to (1.a), but note that Fmap2 is no longer retained in MainMemory.\n",
    "- (2.b) In this mapping, the $P2$ rank is partitioned to create tiles that are rows of feature maps Fmap1, Fmap2, and Fmap3. The tile shape attribute of the loop node implies that the tiles of Fmap3 have shape 1 in the $P2$ rank. LoopTree infers the shape of the tiles of Fmap2 and Fmap1 based on what is required as inputs to compute the specified tile of Fmap3. In this case, tiles that each contains one row of Fmap2 is required to compute tiles of Fmap3, and tiles that each contains one row of Fmap1 is required to compute tiles of Fmap2 in turn.\n",
    "- (2.c) The tiles of Fmap1, Fmap2, and Fmap3 are retained in GlobalBuffer.\n",
    "- (2.d) Similar to before, Fc1 and Fc2 are processed sequentially. However, as we have put node (2.b) above this sequential node, *tiles* of Fc1 and Fc2 are processed sequentially in alternating fashion: a tile of Fc1 produces a tile of Fmap2, the tile of Fmap2 is consumed by a tile of Fc2, another tile of Fc1 is produced, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc558a8-360c-42ab-a62b-80edd1745924",
   "metadata": {},
   "source": [
    "## Running the Model\n",
    "\n",
    "Running with the four matrix multiplications in a row (Test 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87522bd4-a683-46ae-884f-b35f01a557e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytimeloop.looptree.run import run_looptree\n",
    "from util import *\n",
    "from pprint import pp\n",
    "# show_config('architecture.yaml')\n",
    "# As previously mentioned, bindings map levels specified in the mapping\n",
    "# to the hardware units specified in the architecture spec.\n",
    "bindings = {\n",
    "    0: 'MainMemory',\n",
    "    1: 'IntermediateBuffer',\n",
    "    # 2: 'LocalBuffer',\n",
    "    # 3: 'input_reg',\n",
    "    # 4: 'weight_reg',\n",
    "    # 5: 'output_reg',\n",
    "    3: 'MACC'\n",
    "}\n",
    "\n",
    "stats = run_looptree(\n",
    "    CONFIG_DIR,\n",
    "    # ['architecture.yaml', 'two_fc.workload.yaml', 'layer-by-layer.mapping.yaml'],\n",
    "    ['architecture.yaml', 'matrix_multiplication_workload.yaml','matrix_mapping.yaml'],\n",
    "    TMP_DIR,\n",
    "    bindings,\n",
    "    call_accelergy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b6f2040-068f-4505-95aa-cf183f9632a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 8388608\n"
     ]
    }
   ],
   "source": [
    "print('Latency:', stats.latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95b0b1-e942-4362-ab76-818742ac2a78",
   "metadata": {},
   "source": [
    "and also energy, which is computed by calculating the number of actions to each hardware component and multiplying that with the energy per action estimated using the Accelergy tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4427d1fc-aedf-4ee6-8030-5809a2c92596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy:\n",
      "{('MainMemory', 'read'): 167772160.0,\n",
      " ('IntermediateBuffer', 'read'): 3054177943.552,\n",
      " ('IntermediateBuffer', 'write'): 974974713.8560001,\n",
      " ('MainMemory', 'write'): 67108864.0,\n",
      " ('MACC', 'compute'): 7088373.76}\n"
     ]
    }
   ],
   "source": [
    "print('Energy:')\n",
    "pp(stats.energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cd4658f6-4c30-4880-afff-c918a2f58f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.9/250.9 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "542ed82e-22e9-4f4a-a55e-34fa80f9d4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'branches': [[{'rank': 'M', 'tile_shape': 1, 'type': 'spatial'}, {'rank': 'C', 'tile_shape': 256, 'type': 'spatial'}, {'dspace': ['Input'], 'target': 4, 'type': 'storage'}, {'dspace': ['Filter_1'], 'target': 5, 'type': 'storage'}, {'dspace': ['Output'], 'target': 6, 'type': 'storage'}, {'rank': 'Q', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'P', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'C', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'M', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'R', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'S', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'N', 'tile_shape': 1, 'type': 'temporal'}, {'einsum': 'Conv1', 'target': 3, 'type': 'compute'}], [{'rank': 'M2', 'tile_shape': 256, 'type': 'spatial'}, {'rank': 'C2', 'tile_shape': 1, 'type': 'spatial'}, {'dspace': ['Output'], 'target': 4, 'type': 'storage'}, {'dspace': ['Filter_2'], 'target': 5, 'type': 'storage'}, {'dspace': ['Output2'], 'target': 6, 'type': 'storage'}, {'rank': 'M2', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'Q2', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'P2', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'C2', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'R2', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'S2', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'N2', 'tile_shape': 1, 'type': 'temporal'}, {'einsum': 'Conv2', 'target': 3, 'type': 'compute'}]], 'type': 'pipeline'}\n",
      "PE_SIZE, latency, input_r, weights_r, output_r, dram_r, intbuffer_r, input_w, weights_w, output_w, dram_w, intbuffer_w, output_compute\n",
      "1,21990.23255552,76493639670887.88,597606559928.8115,76493603195878.11,73100376932352.0,129633107558.40001,2388579950.592,36475009.76128,76495955834007.72,70368744177664.0,30897457856.512,76493639670887.88\n",
      "2,10995.11627776,76493639670887.88,597606559928.8115,76493603195878.11,73100376932352.0,162296065409.02402,2460961161.216,36475009.76128,76495974071512.6,70368744177664.0,61794915713.024,76493639670887.88\n",
      "3,4810.36337152,76493639670887.88,597606559928.8115,76493603195878.11,73100376932352.0,260284938960.896,2678104793.088,36475009.76128,76496028784027.23,70368744177664.0,154487289282.56,76493639670887.88\n",
      "4,2748.77906944,76493639670887.88,597606559928.8115,76493603195878.11,73100376932352.0,358273812512.768,2895248424.96,36475009.76128,76496083496541.88,70368744177664.0,247179662852.096,76493639670887.88\n",
      "5,1717.9869184,76493639670887.88,597606559928.8115,76493603195878.11,73100376932352.0,521588601765.888,3257154478.08,36475009.76128,76496174684066.28,70368744177664.0,401666952134.656,76493639670887.88\n",
      "6,1202.59084288,76493639670887.88,597606559928.8115,76493603195878.11,73100376932352.0,717566348869.632,3691441741.824,36475009.76128,76496284109095.56,70368744177664.0,587051699273.728,76493639670887.88\n",
      "7,858.9934592,76493639670887.88,597606559928.8115,76493603195878.11,73100376932352.0,946207053824.0,4198110216.192,36475009.76128,76496411771629.73,70368744177664.0,803333904269.312,76493639670887.88\n",
      "8,687.19476736,76493639670887.88,597606559928.8115,76493603195878.11,73100376932352.0,1142184800927.7441,4632397479.936,36475009.76128,76496521196659.02,70368744177664.0,988718651408.384,76493639670887.88\n",
      "9,515.39607552,76493639670887.88,597606559928.8115,76493603195878.11,73100376932352.0,1501477337284.608,5428590796.8,36475009.76128,76496721809212.7,70368744177664.0,1328590687830.016,76493639670887.88\n",
      "10,343.59738368,76493639670887.88,597606559928.8115,76493603195878.11,73100376932352.0,2187399452147.7122,6948596219.904,36475009.76128,76497104796815.2,70368744177664.0,1977437302816.768,76493639670887.88\n",
      "11,343.59738368,76493639670887.88,597606559928.8115,76493603195878.11,73100376932352.0,2187399452147.7122,6948596219.904,36475009.76128,76497104796815.2,70368744177664.0,1977437302816.768,76493639670887.88\n",
      "12,171.79869184,76493639670887.88,597606559928.8115,76493603195878.11,73100376932352.0,4277828754587.648,11580993699.84,36475009.76128,76498271997127.56,70368744177664.0,3954874605633.536,76493639670887.88\n",
      "13,171.79869184,76493639670887.88,597606559928.8115,76493603195878.11,73100376932352.0,4277828754587.648,11580993699.84,36475009.76128,76498271997127.56,70368744177664.0,3954874605633.536,76493639670887.88\n",
      "14,171.79869184,76493639670887.88,597606559928.8115,76493603195878.11,73100376932352.0,4277828754587.648,11580993699.84,36475009.76128,76498271997127.56,70368744177664.0,3954874605633.536,76493639670887.88\n",
      "15,171.79869184,76493639670887.88,597606559928.8115,76493603195878.11,73100376932352.0,4277828754587.648,11580993699.84,36475009.76128,76498271997127.56,70368744177664.0,3954874605633.536,76493639670887.88\n",
      "16,171.79869184,76493639670887.88,597606559928.8115,76493603195878.11,73100376932352.0,4277828754587.648,11580993699.84,36475009.76128,76498271997127.56,70368744177664.0,3954874605633.536,76493639670887.88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### with CONV files (pipeline)\n",
    "from util import *\n",
    "from pprint import pp\n",
    "from pytimeloop.looptree.run import run_looptree\n",
    "import yaml\n",
    "\n",
    "# As previously mentioned, bindings map levels specified in the mapping\n",
    "# to the hardware units specified in the architecture spec.\n",
    "bindings = {\n",
    "    0: 'MainMemory',\n",
    "    1: 'IntermediateBuffer',\n",
    "    # 2: 'LocalBuffer',\n",
    "    # 3: 'input_reg',\n",
    "    # 4: 'weight_reg',\n",
    "    # 5: 'output_reg',\n",
    "    3: 'MACC',\n",
    "    4: 'input_reg',\n",
    "    5: 'weights_reg',\n",
    "    6: 'output_reg',\n",
    "    # 7: 'PE'\n",
    "}\n",
    "\n",
    "#yaml configs\n",
    "\n",
    "class Component(dict): pass\n",
    "class Container(dict): pass\n",
    "\n",
    "# --- YAML Constructors (load from file) ---\n",
    "def component_constructor(loader, node):\n",
    "    return Component(loader.construct_mapping(node))\n",
    "\n",
    "def container_constructor(loader, node):\n",
    "    return Container(loader.construct_mapping(node))\n",
    "\n",
    "# --- YAML Representers (dump to file) ---\n",
    "def component_representer(dumper, data):\n",
    "    return dumper.represent_mapping('!Component', data)\n",
    "\n",
    "def container_representer(dumper, data):\n",
    "    return dumper.represent_mapping('!Container', data)\n",
    "\n",
    "def quoted_str_presenter(dumper, data):\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='\"')\n",
    "\n",
    "yaml.add_representer(str, quoted_str_presenter)\n",
    "\n",
    "# Register constructors\n",
    "yaml.add_constructor('!Component', component_constructor, Loader=yaml.FullLoader)\n",
    "yaml.add_constructor('!Container', container_constructor, Loader=yaml.FullLoader)\n",
    "\n",
    "# Register representers\n",
    "yaml.add_representer(Component, component_representer)\n",
    "yaml.add_representer(Container, container_representer)\n",
    "\n",
    "\n",
    "architecture_file = None\n",
    "conv_mapping_file = None\n",
    "\n",
    "# --- Load YAML ---\n",
    "with open('../configs/architecture.yaml', 'r') as file:\n",
    "    architecture_file = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "with open('../configs/conv_mapping.yaml', 'r') as file:\n",
    "    conv_mapping_file = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "# get the parts of dictionary that define whether we are doing pipelined\n",
    "# and tile_shape of each rank\n",
    "for key in conv_mapping_file[\"mapping\"][\"nodes\"]:\n",
    "    if key['type'] == 'pipeline' or key['type'] == 'sequential':\n",
    "        model = key\n",
    "        pipelined = key['type']\n",
    "        branches = key['branches']\n",
    "        break\n",
    "print(model)\n",
    "# print(branches[1][0])\n",
    "# get PE from dictionary\n",
    "for key in architecture_file[\"architecture\"][\"nodes\"]:\n",
    "    if key['name'] == 'PE':\n",
    "        PE = key\n",
    "        break\n",
    "# print(PE)\n",
    "\n",
    "# pipelined = True\n",
    "\n",
    "# model['type'] = 'pipeline' if pipelined else 'sequential'\n",
    "\n",
    "# C2 = conv_mapping_file[\"mapping\"][\"nodes\"][1]['tile_shape'], can be whatever?\n",
    "# C = branches[0][1], pipelined ? tile_shape = 128/ (0.5 * PE_SIZE**2) : 128/ PE_SIZE**2 \n",
    "# M2 branches[1][0], pipelined ? tile_shape = 128/ (0.5 * PE_SIZE**2) : 128/ PE_SIZE**2\n",
    "\n",
    "# print(conv_mapping_file[\"mapping\"][\"nodes\"][1]['tile_shape'])\n",
    "# print(branches[1][0])\n",
    "\n",
    "numNNEngines = 4\n",
    "final_string = \"PE_SIZE, latency, input_r, weights_r, output_r, dram_r, intbuffer_r, input_w, weights_w, output_w, dram_w, intbuffer_w, output_compute\\n\"\n",
    "for i in range(1, 17):\n",
    "    numPEs = i\n",
    "    PE['spatial']['meshX'] = numPEs\n",
    "    PE['spatial']['meshY'] = numPEs\n",
    "    # PE['attributes']['meshX'] = numPEs\n",
    "    # PE['attributes']['meshY'] = numPEs\n",
    "    \n",
    "    c2_tile_shape= 32\n",
    "    c_tile_shape = tile_shape = int(128/ (0.5 * numPEs**2) if pipelined else 128/ numPEs**2)\n",
    "    m2_tile_shape = tile_shape = int(128/ (0.5 * numPEs**2) if pipelined else 128/ numPEs**2)\n",
    "    # print(c_tile_shape, m2_tile_shape)\n",
    "    \n",
    "    conv_mapping_file[\"mapping\"][\"nodes\"][1]['tile_shape'] = c2_tile_shape\n",
    "    branches[0][1]['tile_shape'] = c_tile_shape\n",
    "    branches[1][0]['tile_shape'] = m2_tile_shape\n",
    "    # print(conv_mapping_file)\n",
    "    \n",
    "    with open('../configs/architecture.yaml', 'w') as file:\n",
    "        yaml.dump(architecture_file, file, default_flow_style=False)\n",
    "    \n",
    "    with open('../configs/conv_mapping.yaml', 'w') as file:\n",
    "        yaml.dump(conv_mapping_file, file, default_flow_style=False)\n",
    "    \n",
    "    \n",
    "    stats = run_looptree(\n",
    "        CONFIG_DIR,\n",
    "        # ['architecture.yaml', 'two_fc.workload.yaml', 'layer-by-layer.mapping.yaml'],\n",
    "        ['architecture.yaml', 'conv_workload.yaml','conv_mapping.yaml'],\n",
    "        TMP_DIR,\n",
    "        bindings,\n",
    "        call_accelergy=True\n",
    "    )\n",
    "    en = stats.energy\n",
    "    final_string += f\"{numPEs},{stats.latency/1e9},{en[('input_reg', 'read')]},{en[('weights_reg', 'read')]},{en[('output_reg', 'read')]},\\\n",
    "{en[('MainMemory', 'read')]},{en[('IntermediateBuffer', 'read')]},{en[('input_reg', 'write')]},{en[('weights_reg', 'write')]},{en[('output_reg', 'write')]},\\\n",
    "{en[('MainMemory', 'write')]},{en[('IntermediateBuffer', 'write')]},{en[('output_reg', 'compute')]}\\n\"\n",
    "\n",
    "import csv\n",
    "\n",
    "# string_data = \"Name,Age,City\\nJohn,30,New York\\nAlice,25,London\\nBob,35,Paris\"\n",
    "\n",
    "with open('peTestPipelineConv.csv', 'w', newline='\\n') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for row in final_string.splitlines():\n",
    "        writer.writerow(row.split(','))\n",
    "\n",
    "print(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "44833dc6-ca07-4c4a-aadb-335e9299db53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PE_SIZE, latency, input_r, weights_r, output_r, dram_r, intbuffer_r, input_w, weights_w, output_w, dram_w, intbuffer_w, output_compute\n",
      "1,35184.372088832,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,129633107558.40001,2388579950.592,36475009.76128,76495955834007.72,    70368744177664.0,30897457856.512,76493639670887.88\n",
      "2,17592.186044416,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,291929172967.424,2460961161.216,36475009.76128,76495974071512.6,    70368744177664.0,61794915713.024,76493639670887.88\n",
      "3,7696.581394432,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,778817369194.496,2678104793.088,36475009.76128,76496028784027.23,    70368744177664.0,154487289282.56,76493639670887.88\n",
      "4,4398.046511104,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,1265705565421.568,2895248424.96,36475009.76128,76496083496541.88,    70368744177664.0,247179662852.096,76493639670887.88\n",
      "5,2748.77906944,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,2077185892466.688,3257154478.08,36475009.76128,76496174684066.28,    70368744177664.0,401666952134.656,76493639670887.88\n",
      "6,1924.145348608,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,3050962284920.832,3691441741.824,36475009.76128,76496284109095.56,    70368744177664.0,587051699273.728,76493639670887.88\n",
      "7,1374.38953472,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,4187034742784.0,4198110216.192,36475009.76128,76496411771629.73,    70368744177664.0,803333904269.312,76493639670887.88\n",
      "8,1099.511627776,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,5160811135238.145,4632397479.936,36475009.76128,76496521196659.02,    70368744177664.0,988718651408.384,76493639670887.88\n",
      "9,824.633720832,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,6946067854737.408,5428590796.8,36475009.76128,76496721809212.7,    70368744177664.0,1328590687830.016,76493639670887.88\n",
      "10,549.755813888,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,10354285228326.912,6948596219.904,36475009.76128,76497104796815.2,    70368744177664.0,1977437302816.768,76493639670887.88\n",
      "11,549.755813888,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,10354285228326.912,6948596219.904,36475009.76128,76497104796815.2,    70368744177664.0,1977437302816.768,76493639670887.88\n",
      "12,274.877906944,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,20741233414504.45,11580993699.84,36475009.76128,76498271997127.56,    70368744177664.0,3954874605633.536,76493639670887.88\n",
      "13,274.877906944,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,20741233414504.45,11580993699.84,36475009.76128,76498271997127.56,    70368744177664.0,3954874605633.536,76493639670887.88\n",
      "14,274.877906944,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,20741233414504.45,11580993699.84,36475009.76128,76498271997127.56,    70368744177664.0,3954874605633.536,76493639670887.88\n",
      "15,274.877906944,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,20741233414504.45,11580993699.84,36475009.76128,76498271997127.56,    70368744177664.0,3954874605633.536,76493639670887.88\n",
      "16,274.877906944,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,20741233414504.45,11580993699.84,36475009.76128,76498271997127.56,    70368744177664.0,3954874605633.536,76493639670887.88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### with CONV files (sequential)\n",
    "from util import *\n",
    "from pprint import pp\n",
    "from pytimeloop.looptree.run import run_looptree\n",
    "import yaml\n",
    "\n",
    "# As previously mentioned, bindings map levels specified in the mapping\n",
    "# to the hardware units specified in the architecture spec.\n",
    "bindings = {\n",
    "    0: 'MainMemory',\n",
    "    1: 'IntermediateBuffer',\n",
    "    # 2: 'LocalBuffer',\n",
    "    # 3: 'input_reg',\n",
    "    # 4: 'weight_reg',\n",
    "    # 5: 'output_reg',\n",
    "    3: 'MACC',\n",
    "    4: 'input_reg',\n",
    "    5: 'weights_reg',\n",
    "    6: 'output_reg',\n",
    "    # 7: 'PE'\n",
    "}\n",
    "\n",
    "#yaml configs\n",
    "\n",
    "class Component(dict): pass\n",
    "class Container(dict): pass\n",
    "\n",
    "# --- YAML Constructors (load from file) ---\n",
    "def component_constructor(loader, node):\n",
    "    return Component(loader.construct_mapping(node))\n",
    "\n",
    "def container_constructor(loader, node):\n",
    "    return Container(loader.construct_mapping(node))\n",
    "\n",
    "# --- YAML Representers (dump to file) ---\n",
    "def component_representer(dumper, data):\n",
    "    return dumper.represent_mapping('!Component', data)\n",
    "\n",
    "def container_representer(dumper, data):\n",
    "    return dumper.represent_mapping('!Container', data)\n",
    "\n",
    "def quoted_str_presenter(dumper, data):\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='\"')\n",
    "\n",
    "yaml.add_representer(str, quoted_str_presenter)\n",
    "\n",
    "# Register constructors\n",
    "yaml.add_constructor('!Component', component_constructor, Loader=yaml.FullLoader)\n",
    "yaml.add_constructor('!Container', container_constructor, Loader=yaml.FullLoader)\n",
    "\n",
    "# Register representers\n",
    "yaml.add_representer(Component, component_representer)\n",
    "yaml.add_representer(Container, container_representer)\n",
    "\n",
    "\n",
    "architecture_file = None\n",
    "conv_mapping_file = None\n",
    "\n",
    "# --- Load YAML ---\n",
    "with open('../configs/architecture.yaml', 'r') as file:\n",
    "    architecture_file = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "with open('../configs/conv_mapping_sequential.yaml', 'r') as file:\n",
    "    conv_mapping_file = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "# # get the parts of dictionary that define whether we are doing pipelined\n",
    "# # and tile_shape of each rank\n",
    "for key in conv_mapping_file[\"mapping\"][\"nodes\"]:\n",
    "    if key['type'] == 'pipeline' or key['type'] == 'sequential':\n",
    "        model = key\n",
    "        pipelined = key['type']\n",
    "        branches = key['branches']\n",
    "        break\n",
    "\n",
    "# print(f\"{branches[1][1]=}, {branches[3][0]=}\")\n",
    "# # for key in branches:\n",
    "#     # print(key)\n",
    "\n",
    "# # print(model)\n",
    "# print(f\"{branches=}\")\n",
    "# # get PE from dictionary\n",
    "for key in architecture_file[\"architecture\"][\"nodes\"]:\n",
    "    if key['name'] == 'PE':\n",
    "        PE = key\n",
    "        break\n",
    "# # print(PE)\n",
    "\n",
    "# # pipelined = True\n",
    "\n",
    "# # model['type'] = 'pipeline' if pipelined else 'sequential'\n",
    "\n",
    "# # C2 = conv_mapping_file[\"mapping\"][\"nodes\"][1]['tile_shape'], can be whatever?\n",
    "# # C = branches[0][1], pipelined ? tile_shape = 128/ (0.5 * PE_SIZE**2) : 128/ PE_SIZE**2 \n",
    "# # M2 branches[1][0], pipelined ? tile_shape = 128/ (0.5 * PE_SIZE**2) : 128/ PE_SIZE**2\n",
    "\n",
    "# # print(conv_mapping_file[\"mapping\"][\"nodes\"][1]['tile_shape'])\n",
    "# # print(branches[1][0])\n",
    "# print(f\"{branches=}\")\n",
    "# print(f\"{branches[0][2]=}\")\n",
    "# print(f\"{branches[1][1]=}\")\n",
    "\n",
    "numNNEngines = 4\n",
    "final_string = \"PE_SIZE, latency, input_r, weights_r, output_r, dram_r, intbuffer_r, input_w, weights_w, output_w, dram_w, intbuffer_w, output_compute\\n\"\n",
    "for i in range(1, 17):\n",
    "    numPEs = i\n",
    "    PE['spatial']['meshX'] = numPEs\n",
    "    PE['spatial']['meshY'] = numPEs\n",
    "    PE['attributes']['meshX'] = numPEs\n",
    "    PE['attributes']['meshY'] = numPEs\n",
    "    \n",
    "    c2_tile_shape= 32\n",
    "    c_tile_shape = tile_shape = int(128/ (0.5 * numPEs**2) if pipelined else 128/ numPEs**2)\n",
    "    m2_tile_shape = tile_shape = int(128/ (0.5 * numPEs**2) if pipelined else 128/ numPEs**2)\n",
    "    # print(c_tile_shape, m2_tile_shape)\n",
    "    \n",
    "    conv_mapping_file[\"mapping\"][\"nodes\"][1]['tile_shape'] = c2_tile_shape\n",
    "    branches[0][2]['tile_shape'] = c_tile_shape\n",
    "    branches[1][0]['tile_shape'] = m2_tile_shape\n",
    "    # print(conv_mapping_file)\n",
    "    \n",
    "    with open('../configs/architecture.yaml', 'w') as file:\n",
    "        yaml.dump(architecture_file, file, default_flow_style=False)\n",
    "    \n",
    "    with open('../configs/conv_mapping_sequential.yaml', 'w') as file:\n",
    "        yaml.dump(conv_mapping_file, file, default_flow_style=False)\n",
    "    \n",
    "    \n",
    "    stats = run_looptree(\n",
    "        CONFIG_DIR,\n",
    "        # ['architecture.yaml', 'two_fc.workload.yaml', 'layer-by-layer.mapping.yaml'],\n",
    "        ['architecture.yaml', 'conv_workload.yaml','conv_mapping_sequential.yaml'],\n",
    "        TMP_DIR,\n",
    "        bindings,\n",
    "        call_accelergy=True\n",
    "    )\n",
    "    en = stats.energy\n",
    "    final_string += f\"{numPEs},{stats.latency/1e9},{en[('input_reg', 'read')]},{en[('weights_reg', 'read')]},{en[('output_reg', 'read')]},\\\n",
    "    {en[('MainMemory', 'read')]},{en[('IntermediateBuffer', 'read')]},{en[('input_reg', 'write')]},{en[('weights_reg', 'write')]},{en[('output_reg', 'write')]},\\\n",
    "    {en[('MainMemory', 'write')]},{en[('IntermediateBuffer', 'write')]},{en[('output_reg', 'compute')]}\\n\"\n",
    "# replace {0} with {en[('IntermediateBuffer', 'read')]} if bringing back intbuffer\n",
    "# same for intbuffer write\n",
    "import csv\n",
    "\n",
    "string_data = \"Name,Age,City\\nJohn,30,New York\\nAlice,25,London\\nBob,35,Paris\"\n",
    "\n",
    "with open('peTestSequentialConv.csv', 'w', newline='\\n') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for row in final_string.splitlines():\n",
    "        writer.writerow(row.split(','))\n",
    "\n",
    "print(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e6d9a69-8826-4d66-bfa1-09c91bbbc68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'instance': '0 <= N < 128 and 0 <= M < 128 and 0 <= P < 128 and 0 <= Q < 128 and 0 <= C < 128 and 0 <= R < 128 and 0 <= S < 128', 'shape': {'data_spaces': [{'dimensions': ['I_batches', 'I_Channels', 'I_Height', 'I_Width'], 'name': 'Input', 'projection': '[ N, C, P + R, Q + S]'}, {'dimensions': ['F_batches', 'F_channels', 'F_Height', 'F_Width'], 'name': 'Filter_1', 'projection': '[ M, C, R, S ]'}, {'dimensions': ['O_in_b', 'O_out_b', 'O_Height', 'O_Width'], 'name': 'Output', 'projection': '[ N, M, P, Q ]', 'read_write': True}], 'dimensions': ['C', 'M', 'R', 'S', 'N', 'P', 'Q'], 'name': 'Conv1'}}, {'instance': '0 <= N2 < 128 and 0 <= M2 < 128 and 0 <= P2 < 128 and 0 <= Q2 < 128 and 0 <= C2 < 128 and 0 <= R2 < 128 and 0 <= S2 < 128', 'shape': {'data_spaces': [{'dimensions': ['I_batches2', 'I_Channels2', 'I_Height2', 'I_Width2'], 'name': 'Output', 'projection': '[ N2, C2, P2 + R2, Q2 + S2]'}, {'dimensions': ['F_batches2', 'F_channels2', 'F_Height2', 'F_Width2'], 'name': 'Filter_2', 'projection': '[ M2, C2, R2, S2 ]'}, {'dimensions': ['O_in_b2', 'O_out_b2', 'O_Height2', 'O_Width2'], 'name': 'Output2', 'projection': '[ N2, M2, P2, Q2 ]', 'read_write': True}], 'dimensions': ['C2', 'M2', 'R2', 'S2', 'N2', 'P2', 'Q2'], 'name': 'Conv2'}}]\n",
      "channels, latency, input_r, weights_r, output_r, dram_r, intbuffer_r, input_w, weights_w, output_w, dram_w, intbuffer_w, output_compute\n",
      "8,68.719476736,298803279964.40576,2334400624.72192,298801000276.29565,    1335751606272.0,79106597838.848,72381210.624,142480.50688,298819237781.17633,    274877906944.0,15448728928.256,298803279964.40576\n",
      "16,68.719476736,1195213119857.623,9337602498.88768,1195208560481.4028,    5411725901824.0,320509261086.72003,289524842.496,569922.02752,1195281510500.9255,    1099511627776.0,61794915713.024,1195213119857.623\n",
      "32,68.719476736,4780852479430.492,37350409995.55072,4780843360678.052,    21784342560768.0,1290202783809.5361,1158099369.984,2279688.11008,4781135160756.143,    4398046511104.0,247179662852.096,4780852479430.492\n",
      "64,137.438953472,19123409917721.97,149401639982.20288,19123391680217.09,    52502217097216.0,5177142614163.456,3474298109.952,9118752.44032,19124558880529.45,    17592186044416.0,988718651408.384,19123409917721.97\n",
      "128,274.877906944,76493639670887.88,597606559928.8115,76493603195878.11,    140738562097152.0,20741233414504.45,11580993699.84,36475009.76128,76498271997127.56,    70368744177664.0,3954874605633.536,76493639670887.88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### with CONV files but for changing the channels, sequential\n",
    "from util import *\n",
    "from pprint import pp\n",
    "from pytimeloop.looptree.run import run_looptree\n",
    "import yaml\n",
    "\n",
    "# As previously mentioned, bindings map levels specified in the mapping\n",
    "# to the hardware units specified in the architecture spec.\n",
    "bindings = {\n",
    "    0: 'MainMemory',\n",
    "    1: 'IntermediateBuffer',\n",
    "    # 2: 'LocalBuffer',\n",
    "    # 3: 'input_reg',\n",
    "    # 4: 'weight_reg',\n",
    "    # 5: 'output_reg',\n",
    "    3: 'MACC',\n",
    "    4: 'input_reg',\n",
    "    5: 'weights_reg',\n",
    "    6: 'output_reg',\n",
    "    # 7: 'PE'\n",
    "}\n",
    "\n",
    "workload_file = None\n",
    "\n",
    "# --- Load YAML ---\n",
    "with open('../configs/conv_workload.yaml', 'r') as file:\n",
    "    workload_file = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "# get the parts of dictionary that define whether we are doing pipelined\n",
    "# and tile_shape of each rank\n",
    "\n",
    "\n",
    "\n",
    "print(workload_file[\"problem\"])\n",
    "\n",
    "final_string = \"channels, latency, input_r, weights_r, output_r, dram_r, intbuffer_r, input_w, weights_w, output_w, dram_w, intbuffer_w, output_compute\\n\"\n",
    "\n",
    "\n",
    "# workload_file[\"problem\"][\n",
    "\n",
    "for i in range(3, 8):\n",
    "    layer_one_input_channels = 2**i\n",
    "    layer_two_input_channels = 2**i\n",
    "    layer_two_output_channels = 2**i\n",
    "    \n",
    "    instance_string_one = f\"0 <= N < 128 and 0 <= M < {layer_two_input_channels} and 0 <= P < 128 and 0 <= Q < 128 and 0 <= C < {layer_one_input_channels} and 0 <= R < 128 and 0 <= S < 128\"\n",
    "    \n",
    "    instance_string_two = f\"0 <= N2 < 128 and 0 <= M2 < {layer_two_output_channels} and 0 <= P2 < 128 and 0 <= Q2 < 128 and 0 <= C2 < {layer_two_input_channels} and 0 <= R2 < 128 and 0 <= S2 < 128\"\n",
    "    \n",
    "    for key in workload_file[\"problem\"]:\n",
    "        name = key['shape']['name']\n",
    "        # print(key)\n",
    "        if name == 'Conv1':\n",
    "            key['instance'] = instance_string_one\n",
    "        else:\n",
    "            key['instance'] = instance_string_two\n",
    "\n",
    "    with open('../configs/conv_workload.yaml', 'w') as file:\n",
    "        yaml.dump(workload_file, file, default_flow_style=False)\n",
    "\n",
    "    stats = run_looptree(\n",
    "        CONFIG_DIR,\n",
    "        # ['architecture.yaml', 'two_fc.workload.yaml', 'layer-by-layer.mapping.yaml'],\n",
    "        ['architecture.yaml', 'conv_workload.yaml','conv_mapping_sequential.yaml'],\n",
    "        TMP_DIR,\n",
    "        bindings,\n",
    "        call_accelergy=True\n",
    "    )\n",
    "    en = stats.energy\n",
    "    final_string += f\"{2**i},{stats.latency/1e9},{en[('input_reg', 'read')]},{en[('weights_reg', 'read')]},{en[('output_reg', 'read')]},\\\n",
    "    {en[('MainMemory', 'read')]},{en[('IntermediateBuffer', 'read')]},{en[('input_reg', 'write')]},{en[('weights_reg', 'write')]},{en[('output_reg', 'write')]},\\\n",
    "    {en[('MainMemory', 'write')]},{en[('IntermediateBuffer', 'write')]},{en[('output_reg', 'compute')]}\\n\"\n",
    "\n",
    "import csv\n",
    "\n",
    "# string_data = \"Name,Age,City\\nJohn,30,New York\\nAlice,25,London\\nBob,35,Paris\"\n",
    "\n",
    "with open('channelTestSequentialConv.csv', 'w', newline='\\n') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for row in final_string.splitlines():\n",
    "        writer.writerow(row.split(','))\n",
    "\n",
    "print(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e08e6207-4960-4d77-b891-1f728adf557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'instance': '0 <= N < 128 and 0 <= M < 128 and 0 <= P < 128 and 0 <= Q < 128 and 0 <= C < 128 and 0 <= R < 128 and 0 <= S < 128', 'shape': {'data_spaces': [{'dimensions': ['I_batches', 'I_Channels', 'I_Height', 'I_Width'], 'name': 'Input', 'projection': '[ N, C, P + R, Q + S]'}, {'dimensions': ['F_batches', 'F_channels', 'F_Height', 'F_Width'], 'name': 'Filter_1', 'projection': '[ M, C, R, S ]'}, {'dimensions': ['O_in_b', 'O_out_b', 'O_Height', 'O_Width'], 'name': 'Output', 'projection': '[ N, M, P, Q ]', 'read_write': True}], 'dimensions': ['C', 'M', 'R', 'S', 'N', 'P', 'Q'], 'name': 'Conv1'}}, {'instance': '0 <= N2 < 128 and 0 <= M2 < 128 and 0 <= P2 < 128 and 0 <= Q2 < 128 and 0 <= C2 < 128 and 0 <= R2 < 128 and 0 <= S2 < 128', 'shape': {'data_spaces': [{'dimensions': ['I_batches2', 'I_Channels2', 'I_Height2', 'I_Width2'], 'name': 'Output', 'projection': '[ N2, C2, P2 + R2, Q2 + S2]'}, {'dimensions': ['F_batches2', 'F_channels2', 'F_Height2', 'F_Width2'], 'name': 'Filter_2', 'projection': '[ M2, C2, R2, S2 ]'}, {'dimensions': ['O_in_b2', 'O_out_b2', 'O_Height2', 'O_Width2'], 'name': 'Output2', 'projection': '[ N2, M2, P2, Q2 ]', 'read_write': True}], 'dimensions': ['C2', 'M2', 'R2', 'S2', 'N2', 'P2', 'Q2'], 'name': 'Conv2'}}]\n",
      "channels, latency, input_r, weights_r, output_r, dram_r, intbuffer_r, input_w, weights_w, output_w, dram_w, intbuffer_w, output_compute\n",
      "8,68.719476736,298803279964.40576,2334400624.72192,298801000276.29565,    381180444672.0,22392113282.048,72381210.624,142480.50688,298819237781.17633,    274877906944.0,15448728928.256,298803279964.40576\n",
      "16,68.719476736,1195213119857.623,9337602498.88768,1195208560481.4028,    1320706637824.0,77447184414.72,289524842.496,569922.02752,1195281510500.9255,    1099511627776.0,61794915713.024,1195213119857.623\n",
      "32,68.719476736,4780852479430.492,37350409995.55072,4780843360678.052,    4874796269568.0,285546200231.93604,1158099369.984,2279688.11008,4781135160756.143,    4398046511104.0,247179662852.096,4780852479430.492\n",
      "64,103.079215104,19123409917721.97,149401639982.20288,19123391680217.09,    18683124514816.0,1093699726073.8561,3474298109.952,9118752.44032,19124558880529.45,    17592186044416.0,988718651408.384,19123409917721.97\n",
      "128,171.79869184,76493639670887.88,597606559928.8115,76493603195878.11,    73100376932352.0,4277828754587.648,11580993699.84,36475009.76128,76498271997127.56,    70368744177664.0,3954874605633.536,76493639670887.88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### with CONV files but for changing the channels, pipeline\n",
    "from util import *\n",
    "from pprint import pp\n",
    "from pytimeloop.looptree.run import run_looptree\n",
    "import yaml\n",
    "\n",
    "# As previously mentioned, bindings map levels specified in the mapping\n",
    "# to the hardware units specified in the architecture spec.\n",
    "bindings = {\n",
    "    0: 'MainMemory',\n",
    "    1: 'IntermediateBuffer',\n",
    "    # 2: 'LocalBuffer',\n",
    "    # 3: 'input_reg',\n",
    "    # 4: 'weight_reg',\n",
    "    # 5: 'output_reg',\n",
    "    3: 'MACC',\n",
    "    4: 'input_reg',\n",
    "    5: 'weights_reg',\n",
    "    6: 'output_reg',\n",
    "    # 7: 'PE'\n",
    "}\n",
    "\n",
    "workload_file = None\n",
    "\n",
    "# --- Load YAML ---\n",
    "with open('../configs/conv_workload.yaml', 'r') as file:\n",
    "    workload_file = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "# get the parts of dictionary that define whether we are doing pipelined\n",
    "# and tile_shape of each rank\n",
    "\n",
    "\n",
    "\n",
    "print(workload_file[\"problem\"])\n",
    "\n",
    "final_string = \"channels, latency, input_r, weights_r, output_r, dram_r, intbuffer_r, input_w, weights_w, output_w, dram_w, intbuffer_w, output_compute\\n\"\n",
    "\n",
    "\n",
    "# workload_file[\"problem\"][\n",
    "\n",
    "for i in range(3, 8):\n",
    "    layer_one_input_channels = 2**i\n",
    "    layer_two_input_channels = 2**i\n",
    "    layer_two_output_channels = 2**i\n",
    "    \n",
    "    instance_string_one = f\"0 <= N < 128 and 0 <= M < {layer_two_input_channels} and 0 <= P < 128 and 0 <= Q < 128 and 0 <= C < {layer_one_input_channels} and 0 <= R < 128 and 0 <= S < 128\"\n",
    "    \n",
    "    instance_string_two = f\"0 <= N2 < 128 and 0 <= M2 < {layer_two_output_channels} and 0 <= P2 < 128 and 0 <= Q2 < 128 and 0 <= C2 < {layer_two_input_channels} and 0 <= R2 < 128 and 0 <= S2 < 128\"\n",
    "    \n",
    "    for key in workload_file[\"problem\"]:\n",
    "        name = key['shape']['name']\n",
    "        # print(key)\n",
    "        if name == 'Conv1':\n",
    "            key['instance'] = instance_string_one\n",
    "        else:\n",
    "            key['instance'] = instance_string_two\n",
    "\n",
    "    with open('../configs/conv_workload.yaml', 'w') as file:\n",
    "        yaml.dump(workload_file, file, default_flow_style=False)\n",
    "\n",
    "    stats = run_looptree(\n",
    "        CONFIG_DIR,\n",
    "        # ['architecture.yaml', 'two_fc.workload.yaml', 'layer-by-layer.mapping.yaml'],\n",
    "        ['architecture.yaml', 'conv_workload.yaml','conv_mapping.yaml'],\n",
    "        TMP_DIR,\n",
    "        bindings,\n",
    "        call_accelergy=True\n",
    "    )\n",
    "    en = stats.energy\n",
    "    final_string += f\"{2**i},{stats.latency/1e9},{en[('input_reg', 'read')]},{en[('weights_reg', 'read')]},{en[('output_reg', 'read')]},\\\n",
    "    {en[('MainMemory', 'read')]},{en[('IntermediateBuffer', 'read')]},{en[('input_reg', 'write')]},{en[('weights_reg', 'write')]},{en[('output_reg', 'write')]},\\\n",
    "    {en[('MainMemory', 'write')]},{en[('IntermediateBuffer', 'write')]},{en[('output_reg', 'compute')]}\\n\"\n",
    "\n",
    "import csv\n",
    "\n",
    "# string_data = \"Name,Age,City\\nJohn,30,New York\\nAlice,25,London\\nBob,35,Paris\"\n",
    "\n",
    "with open('channelTestPipelineConv.csv', 'w', newline='\\n') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for row in final_string.splitlines():\n",
    "        writer.writerow(row.split(','))\n",
    "\n",
    "print(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "c1d268ec-fbe4-43f6-9482-7e46cd056b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'branches': [[{'rank': 'M', 'tile_shape': 1, 'type': 'spatial'}, {'rank': 'C', 'tile_shape': 1, 'type': 'spatial'}, {'dspace': ['Input'], 'target': 4, 'type': 'storage'}, {'dspace': ['Filter_1'], 'target': 5, 'type': 'storage'}, {'dspace': ['Output'], 'target': 6, 'type': 'storage'}, {'rank': 'Q', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'P', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'C', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'M', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'R', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'S', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'N', 'tile_shape': 1, 'type': 'temporal'}, {'einsum': 'Conv1', 'target': 3, 'type': 'compute'}], [{'rank': 'M2', 'tile_shape': 1, 'type': 'spatial'}, {'rank': 'C2', 'tile_shape': 1, 'type': 'spatial'}, {'dspace': ['Output'], 'target': 4, 'type': 'storage'}, {'dspace': ['Filter_2'], 'target': 5, 'type': 'storage'}, {'dspace': ['Output2'], 'target': 6, 'type': 'storage'}, {'rank': 'M2', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'Q2', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'P2', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'C2', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'R2', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'S2', 'tile_shape': 1, 'type': 'temporal'}, {'rank': 'N2', 'tile_shape': 1, 'type': 'temporal'}, {'einsum': 'Conv2', 'target': 3, 'type': 'compute'}]], 'type': 'pipeline'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[273], line 119\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../configs/new_matrix_mapping.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    116\u001b[0m     yaml\u001b[38;5;241m.\u001b[39mdump(conv_mapping_file, file, default_flow_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 119\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mrun_looptree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCONFIG_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ['architecture.yaml', 'two_fc.workload.yaml', 'layer-by-layer.mapping.yaml'],\u001b[39;49;00m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marchitecture.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnew_matrix_workload.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnew_matrix_mapping.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTMP_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcall_accelergy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    126\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m en \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39menergy\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# print(en)\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# no more intbuffer because of matrix???\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytimeloop/looptree/run.py:48\u001b[0m, in \u001b[0;36mrun_looptree\u001b[0;34m(config_dir, paths, tmp_path, bindings, call_accelergy)\u001b[0m\n\u001b[1;32m     43\u001b[0m     call_accelergy_verbose(spec, tmp_path)\n\u001b[1;32m     44\u001b[0m     spec \u001b[38;5;241m=\u001b[39m Specification\u001b[38;5;241m.\u001b[39mfrom_yaml_files([\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mstr\u001b[39m(config_dir \u001b[38;5;241m/\u001b[39m p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths\n\u001b[1;32m     46\u001b[0m     ] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mstr\u001b[39m(Path(tmp_path) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERT.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[0;32m---> 48\u001b[0m result \u001b[38;5;241m=\u001b[39m deserialize_looptree_output(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, isl\u001b[38;5;241m.\u001b[39mDEFAULT_CONTEXT)\n\u001b[1;32m     50\u001b[0m actions \u001b[38;5;241m=\u001b[39m gather_actions(result, spec\u001b[38;5;241m.\u001b[39mmapping, workload, bindings)\n\u001b[1;32m     51\u001b[0m energy \u001b[38;5;241m=\u001b[39m compute_energy_from_actions(actions, spec\u001b[38;5;241m.\u001b[39mERT)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### with NEW MATRIX files\n",
    "from util import *\n",
    "from pprint import pp\n",
    "from pytimeloop.looptree.run import run_looptree\n",
    "import yaml\n",
    "\n",
    "# As previously mentioned, bindings map levels specified in the mapping\n",
    "# to the hardware units specified in the architecture spec.\n",
    "bindings = {\n",
    "    0: 'MainMemory',\n",
    "    1: 'IntermediateBuffer',\n",
    "    # 2: 'LocalBuffer',\n",
    "    # 3: 'input_reg',\n",
    "    # 4: 'weight_reg',\n",
    "    # 5: 'output_reg',\n",
    "    3: 'MACC',\n",
    "    4: 'input_reg',\n",
    "    5: 'weights_reg',\n",
    "    6: 'output_reg',\n",
    "    # 7: 'PE'\n",
    "}\n",
    "\n",
    "#yaml configs\n",
    "\n",
    "class Component(dict): pass\n",
    "class Container(dict): pass\n",
    "\n",
    "# --- YAML Constructors (load from file) ---\n",
    "def component_constructor(loader, node):\n",
    "    return Component(loader.construct_mapping(node))\n",
    "\n",
    "def container_constructor(loader, node):\n",
    "    return Container(loader.construct_mapping(node))\n",
    "\n",
    "# --- YAML Representers (dump to file) ---\n",
    "def component_representer(dumper, data):\n",
    "    return dumper.represent_mapping('!Component', data)\n",
    "\n",
    "def container_representer(dumper, data):\n",
    "    return dumper.represent_mapping('!Container', data)\n",
    "\n",
    "\n",
    "\n",
    "# Register constructors\n",
    "yaml.add_constructor('!Component', component_constructor, Loader=yaml.FullLoader)\n",
    "yaml.add_constructor('!Container', container_constructor, Loader=yaml.FullLoader)\n",
    "\n",
    "# Register representers\n",
    "yaml.add_representer(Component, component_representer)\n",
    "yaml.add_representer(Container, container_representer)\n",
    "\n",
    "\n",
    "architecture_file = None\n",
    "conv_mapping_file = None\n",
    "\n",
    "# --- Load YAML ---\n",
    "with open('../configs/architecture.yaml', 'r') as file:\n",
    "    architecture_file = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "with open('../configs/new_matrix_mapping.yaml', 'r') as file:\n",
    "    conv_mapping_file = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "# get the parts of dictionary that define whether we are doing pipelined\n",
    "# and tile_shape of each rank\n",
    "for key in conv_mapping_file[\"mapping\"][\"nodes\"]:\n",
    "    if key['type'] == 'pipeline' or key['type'] == 'sequential':\n",
    "        model = key\n",
    "        pipelined = key['type']\n",
    "        branches = key['branches']\n",
    "        break\n",
    "print(model)\n",
    "# print(branches[1][0])\n",
    "# get PE from dictionary\n",
    "for key in architecture_file[\"architecture\"][\"nodes\"]:\n",
    "    if key['name'] == 'PE':\n",
    "        PE = key\n",
    "        break\n",
    "# print(PE)\n",
    "\n",
    "# pipelined = True\n",
    "\n",
    "# model['type'] = 'pipeline' if pipelined else 'sequential'\n",
    "\n",
    "# C2 = conv_mapping_file[\"mapping\"][\"nodes\"][1]['tile_shape'], can be whatever?\n",
    "# C = branches[0][1], pipelined ? tile_shape = 128/ (0.5 * PE_SIZE**2) : 128/ PE_SIZE**2 \n",
    "# M2 branches[1][0], pipelined ? tile_shape = 128/ (0.5 * PE_SIZE**2) : 128/ PE_SIZE**2\n",
    "\n",
    "# print(conv_mapping_file[\"mapping\"][\"nodes\"][1]['tile_shape'])\n",
    "# print(branches[1][0])\n",
    "\n",
    "numNNEngines = 4\n",
    "\n",
    "#no more intbuffer because of matrix???\n",
    "final_string = \"PE_SIZE, latency, input_r, weights_r, output_r, dram_r, intbuffer_r, input_w, weights_w, output_w, dram_w, output_compute\\n\"\n",
    "for i in range(1, 17):\n",
    "    numPEs = i\n",
    "    PE['spatial']['meshX'] = numPEs\n",
    "    PE['spatial']['meshY'] = numPEs\n",
    "    # PE['attributes']['meshX'] = numPEs\n",
    "    # PE['attributes']['meshY'] = numPEs\n",
    "    \n",
    "    c2_tile_shape= 32\n",
    "    c_tile_shape = tile_shape = int(128/ (0.5 * numPEs**2) if pipelined else 128/ numPEs**2)\n",
    "    m2_tile_shape = tile_shape = int(128/ (0.5 * numPEs**2) if pipelined else 128/ numPEs**2)\n",
    "    # print(c_tile_shape, m2_tile_shape)\n",
    "    \n",
    "    conv_mapping_file[\"mapping\"][\"nodes\"][1]['tile_shape'] = c2_tile_shape\n",
    "    branches[0][1]['tile_shape'] = c_tile_shape\n",
    "    branches[1][0]['tile_shape'] = m2_tile_shape\n",
    "    # print(conv_mapping_file)\n",
    "    \n",
    "    with open('../configs/architecture.yaml', 'w') as file:\n",
    "        yaml.dump(architecture_file, file, default_flow_style=False)\n",
    "    \n",
    "    with open('../configs/new_matrix_mapping.yaml', 'w') as file:\n",
    "        yaml.dump(conv_mapping_file, file, default_flow_style=False)\n",
    "    \n",
    "    \n",
    "    stats = run_looptree(\n",
    "        CONFIG_DIR,\n",
    "        # ['architecture.yaml', 'two_fc.workload.yaml', 'layer-by-layer.mapping.yaml'],\n",
    "        ['architecture.yaml', 'new_matrix_workload.yaml','new_matrix_mapping.yaml'],\n",
    "        TMP_DIR,\n",
    "        bindings,\n",
    "        call_accelergy=True\n",
    "    )\n",
    "    en = stats.energy\n",
    "    # print(en)\n",
    "    # no more intbuffer because of matrix???\n",
    "    final_string += f\"{numPEs},{stats.latency/1e3},{en[('input_reg', 'read')]},{en[('weights_reg', 'read')]},{en[('output_reg', 'read')]},\\\n",
    "{en[('MainMemory', 'read')]},{en[('IntermediateBuffer', 'read')]},{en[('input_reg', 'write')]},{en[('weights_reg', 'write')]},{en[('output_reg', 'write')]},\\\n",
    "{en[('MainMemory', 'write')]},{en[('output_reg', 'compute')]}\\n\"\n",
    "\n",
    "import csv\n",
    "\n",
    "# string_data = \"Name,Age,City\\nJohn,30,New York\\nAlice,25,London\\nBob,35,Paris\"\n",
    "\n",
    "with open('peTestPipelineMatrix.csv', 'w', newline='\\n') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for row in final_string.splitlines():\n",
    "        writer.writerow(row.split(','))\n",
    "\n",
    "print(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a68d8dfd-73be-41d7-a0aa-0bb0e9c91b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 549.755813888\n",
      "Energy:\n",
      "{('input_reg', 'read'): 76493639670887.88,\n",
      " ('weights_reg', 'read'): 597606559928.8115,\n",
      " ('output_reg', 'read'): 76493603195878.11,\n",
      " ('MainMemory', 'read'): 73100376932352.0,\n",
      " ('IntermediateBuffer', 'read'): 2187399452147.7122,\n",
      " ('output_reg', 'write'): 76497104796815.2,\n",
      " ('input_reg', 'write'): 6948596219.904,\n",
      " ('weights_reg', 'write'): 36475009.76128,\n",
      " ('IntermediateBuffer', 'write'): 1977437302816.768,\n",
      " ('MainMemory', 'write'): 70368744177664.0,\n",
      " ('output_reg', 'compute'): 76493639670887.88}\n"
     ]
    }
   ],
   "source": [
    "print('Latency:', stats.latency/1e9)\n",
    "print('Energy:')\n",
    "pp(stats.energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f74c4e6b-5e96-4ef8-aa1c-19592b1de325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoopTreeStatistics(latency=687194767360,\n",
      "                   energy={('input_reg', 'read'): 76493639670887.88,\n",
      "                           ('weights_reg', 'read'): 597606559928.8115,\n",
      "                           ('output_reg', 'read'): 76493603195878.11,\n",
      "                           ('MainMemory', 'read'): 73100376932352.0,\n",
      "                           ('IntermediateBuffer', 'read'): 1142184800927.7441,\n",
      "                           ('output_reg', 'write'): 76496521196659.02,\n",
      "                           ('input_reg', 'write'): 4632397479.936,\n",
      "                           ('weights_reg', 'write'): 36475009.76128,\n",
      "                           ('IntermediateBuffer', 'write'): 988718651408.384,\n",
      "                           ('MainMemory', 'write'): 70368744177664.0,\n",
      "                           ('output_reg', 'compute'): 76493639670887.88},\n",
      "                   actions={('input_reg', 'read'): 1125899906842624,\n",
      "                            ('weights_reg', 'read'): 8796093022208,\n",
      "                            ('output_reg', 'read'): 1125899369971712.0,\n",
      "                            ('MainMemory', 'read'): 35693543424.0,\n",
      "                            ('IntermediateBuffer', 'read'): 9386868736.0,\n",
      "                            ('MACC', 'read'): 0,\n",
      "                            ('output_reg', 'write'): 1125942319644672.0,\n",
      "                            ('input_reg', 'write'): 68183654400,\n",
      "                            ('weights_reg', 'write'): 536870912,\n",
      "                            ('IntermediateBuffer', 'write'): 8589934592,\n",
      "                            ('MainMemory', 'write'): 34359738368,\n",
      "                            ('MACC', 'write'): 0,\n",
      "                            ('output_reg', 'compute'): 1125899906842624},\n",
      "                   memory_latency={'MainMemory': 35406335.96680656,\n",
      "                                   'IntermediateBuffer': 33817087.968296476,\n",
      "                                   'MACC': 0.0,\n",
      "                                   'input_reg': 137447276156.5727,\n",
      "                                   'weights_reg': 1073807355.9732223,\n",
      "                                   'output_reg': 274883148793.1882},\n",
      "                   capacity_usage={'MainMemory': Val(\"1870675968\"),\n",
      "                                   'IntermediateBuffer': Val(\"266342400\"),\n",
      "                                   'input_reg': Val(\"41616000\"),\n",
      "                                   'weights_reg': Val(\"131072\"),\n",
      "                                   'output_reg': Val(\"10485760\")})\n"
     ]
    }
   ],
   "source": [
    "pp(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cbe63c-494f-48fc-b341-317ebb2123d0",
   "metadata": {},
   "source": [
    "Now, we compare these results with the fused mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e339a0d-e596-4145-aa2a-4a77faf7aebf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CONFIG_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# As previously mentioned, bindings map levels specified in the mapping\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# to the hardware units specified in the architecture spec.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m bindings \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMainMemory\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGlobalBuffer\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMACC\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     11\u001b[0m stats \u001b[38;5;241m=\u001b[39m run_looptree(\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mCONFIG_DIR\u001b[49m,\n\u001b[1;32m     13\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchitecture.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_fc.workload.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused.mapping.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     14\u001b[0m     TMP_DIR,\n\u001b[1;32m     15\u001b[0m     bindings,\n\u001b[1;32m     16\u001b[0m     call_accelergy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatency:\u001b[39m\u001b[38;5;124m'\u001b[39m, stats\u001b[38;5;241m.\u001b[39mlatency)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnergy:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CONFIG_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "from pytimeloop.looptree.run import run_looptree\n",
    "\n",
    "# As previously mentioned, bindings map levels specified in the mapping\n",
    "# to the hardware units specified in the architecture spec.\n",
    "bindings = {\n",
    "    0: 'MainMemory',\n",
    "    1: 'GlobalBuffer',\n",
    "    2: 'MACC'\n",
    "}\n",
    "\n",
    "stats = run_looptree(\n",
    "    CONFIG_DIR,\n",
    "    ['architecture.yaml', 'two_fc.workload.yaml', 'fused.mapping.yaml'],\n",
    "    TMP_DIR,\n",
    "    bindings,\n",
    "    call_accelergy=True\n",
    ")\n",
    "print('Latency:', stats.latency)\n",
    "print('Energy:')\n",
    "pp(stats.energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e41bc-8a1e-4f57-a95f-c8bdd275731d",
   "metadata": {},
   "source": [
    "As we can see, the energy consumption due to MainMemory reads and writes have decreased significantly from fusion.\n",
    "\n",
    "Here, we have not modeled the impact of limited MainMemory bandwidth; thus, because the MACC unit utilization is the same (100%) in both cases, the latency is the same. However, if the layer-by-layer latency is limited by MainMemory bandwidth, fusion will also decrease latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "574293da-bca2-4383-820a-438468a50572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 528384\n",
      "Energy:\n",
      "{('MainMemory', 'read'): 50331648.0,\n",
      " ('GlobalBuffer', 'read'): 382769037.31200004,\n",
      " ('GlobalBuffer', 'write'): 123521941.50400001,\n",
      " ('MainMemory', 'write'): 16777216.0,\n",
      " ('MACC', 'compute'): 886046.72}\n"
     ]
    }
   ],
   "source": [
    "from pytimeloop.looptree.run import run_looptree\n",
    "\n",
    "# As previously mentioned, bindings map levels specified in the mapping\n",
    "# to the hardware units specified in the architecture spec.\n",
    "bindings = {\n",
    "    0: 'MainMemory',\n",
    "    1: 'GlobalBuffer',\n",
    "    2: 'MACC'\n",
    "}\n",
    "\n",
    "stats = run_looptree(\n",
    "    CONFIG_DIR,\n",
    "    ['architecture.yaml', 'two_fc.workload.yaml', 'fused-pipeline.mapping.yaml'],\n",
    "    TMP_DIR,\n",
    "    bindings,\n",
    "    call_accelergy=True\n",
    ")\n",
    "print('Latency:', stats.latency)\n",
    "print('Energy:')\n",
    "pp(stats.energy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
